<repo-to-text>
Directory: other

Directory Structure:
<directory_structure>
.
.
├── ./BICOExplorer.cu
├── ./BICOExplorer.h
├── ./CMakeLists.txt
├── ./Evaluator.cu
├── ./Evaluator.h
├── ./KernelConfig.h
├── ./kernel.cuh
├── ./main.cu
└── ./myfile.txt

</directory_structure>

<content full_path="Evaluator.h">
#pragma once
#include "KernelConfig.h"
#include <cuda_runtime.h>

class Evaluator {
public:
  Evaluator(float *d_A, const float *d_B, float *d_C, int M, int N, int K);
  double evaluate(const KernelConfig &config);

private:
  float *d_A, *d_B, *d_C;
  int M, N, K;
};

</content>

<content full_path="kernel.cuh">
#pragma once
#include <cuda_runtime.h>

template <int TILE_M, int TILE_N, int TILE_K, int BLOCK_DIM_X, int BLOCK_DIM_Y>
__global__ void fused_glu_kernel(float *C, const float *A, const float *B,
                                 int M, int N, int K) {
  extern __shared__ float smem[];
  float *As = smem;
  float *Bs = smem + TILE_M * TILE_K;

  const int num_threads = BLOCK_DIM_X * BLOCK_DIM_Y;
  int tx = threadIdx.x;
  int ty = threadIdx.y;
  int bx = blockIdx.x;
  int by = blockIdx.y;

  int row = by * TILE_M + ty;
  int col = bx * TILE_N + tx;

  float sum = 0.0f;
  if (row < M && col < N) {
    int tk_step = TILE_K;
    for (int tk = 0; tk < K; tk += tk_step) {
      // Load A tile
      for (int i = ty * BLOCK_DIM_X + tx; i < TILE_M * TILE_K;
           i += num_threads) {
        int ii = i / TILE_K;
        int jj = i % TILE_K;
        int r = by * TILE_M + ii;
        int c = tk + jj;
        As[i] = (r < M && c < K) ? A[r * K + c] : 0.0f;
      }
      // Load B tile
      for (int i = ty * BLOCK_DIM_X + tx; i < TILE_K * TILE_N;
           i += num_threads) {
        int ii = i / TILE_N;
        int jj = i % TILE_N;
        int r = tk + ii;
        int c = bx * TILE_N + jj;
        Bs[i] = (r < K && c < N) ? B[r * N + c] : 0.0f;
      }
      __syncthreads();
      // Compute
      for (int k = 0; k < TILE_K; ++k) {
        if (tk + k < K) {
          sum += As[ty * TILE_K + k] * Bs[k * TILE_N + tx];
        }
      }
      __syncthreads();
    }
    C[row * N + col] = sum;
  }
}

</content>

<content full_path="BICOExplorer.h">
#pragma once
#include "Evaluator.h"
#include "KernelConfig.h"
#include <vector>

class BICOExplorer {
private:
  std::vector<KernelConfig> exploration_space_;
  double best_latency_;
  KernelConfig best_config_;
  std::vector<KernelConfig> information_sink_;
  Evaluator evaluator_;

public:
  BICOExplorer(std::vector<KernelConfig> search_space, Evaluator evaluator);
  void explore(int max_budget);
};

</content>

<content full_path="CMakeLists.txt">
cmake_minimum_required(VERSION 3.18)
project(MLPBenchMarkBICO LANGUAGES CXX CUDA)
if(POLICY CMP0104)
  cmake_policy(SET CMP0104 NEW)
endif()
find_package(CUDAToolkit REQUIRED)
set(CMAKE_CUDA_ARCHITECTURES 90)
set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} --expt-relaxed-constexpr")
set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} --use_fast_math")
add_executable(mlp_benchmark
    main.cu
    Evaluator.cu
    BICOExplorer.cu
)
target_link_libraries(mlp_benchmark PRIVATE CUDA::cudart)
set_property(TARGET mlp_benchmark PROPERTY CXX_STANDARD 17)
set_property(TARGET mlp_benchmark PROPERTY CUDA_STANDARD 17)
target_compile_options(mlp_benchmark PRIVATE
    $<$<COMPILE_LANGUAGE:CUDA>:
        -Xcompiler=-Wall
        -Xcompiler=-Wextra
        --generate-line-info
    >
)
target_include_directories(mlp_benchmark PRIVATE
    ${CMAKE_CURRENT_SOURCE_DIR}
)
message(STATUS "CUDA architecture set to: ${CMAKE_CUDA_ARCHITECTURES}")
message(STATUS "CUDA toolkit version: ${CUDAToolkit_VERSION}")
message(STATUS "")
message(STATUS "Build instructions:")
message(STATUS " mkdir -p build && cd build")
message(STATUS " cmake ..")
message(STATUS " make -j")
message(STATUS "")
message(STATUS "Run with:")
message(STATUS " ./mlp_benchmark")

</content>

<content full_path="KernelConfig.h">
#pragma once
#include <sstream>
#include <string>

struct KernelConfig {
  // For simplicity, we use a few key parameters.
  // These could be template parameters for max performance.
  int TILE_M;
  int TILE_N;
  int TILE_K;
  int BLOCK_DIM_X;
  int BLOCK_DIM_Y;
  // Helper for printing
  std::string toString() const {
    std::stringstream ss;
    ss << "TM=" << TILE_M << ", TN=" << TILE_N << ", TK=" << TILE_K
       << ", BX=" << BLOCK_DIM_X << ", BY=" << BLOCK_DIM_Y;
    return ss.str();
  }
};

</content>

<content full_path="myfile.txt">

</content>

<content full_path="main.cu">
#include "BICOExplorer.h"
#include "Evaluator.h"
#include "KernelConfig.h"
#include "kernel.cuh"
#include <cuda_runtime.h>
#include <iostream>
#include <random>
#include <vector>

// Helper macro for checking CUDA calls
#define CUDA_CHECK(call)                                                       \
  do {                                                                         \
    cudaError_t err = call;                                                    \
    if (err != cudaSuccess) {                                                  \
      fprintf(stderr, "CUDA Error at %s:%d: %s\n", __FILE__, __LINE__,         \
              cudaGetErrorString(err));                                        \
      exit(EXIT_FAILURE);                                                      \
    }                                                                          \
  } while (0)

int main() {
  const int M = 1024;
  const int K = 4096;
  const int N = 12288;

  std::cout << "Matrix dimensions: " << M << " x " << K << " * " << K << " x "
            << N << std::endl;

  // Generate random data on host
  std::vector<float> h_A(M * K);
  std::vector<float> h_B(K * N);
  std::vector<float> h_C(M * N);
  std::mt19937 rng(1337);
  std::uniform_real_distribution<float> dist(-1.0f, 1.0f);
  for (auto &val : h_A)
    val = dist(rng);
  for (auto &val : h_B)
    val = dist(rng);

  // Allocate device memory
  float *d_A, *d_B, *d_C;
  CUDA_CHECK(cudaMalloc(&d_A, M * K * sizeof(float)));
  CUDA_CHECK(cudaMalloc(&d_B, K * N * sizeof(float)));
  CUDA_CHECK(cudaMalloc(&d_C, M * N * sizeof(float)));
  CUDA_CHECK(cudaMemcpy(d_A, h_A.data(), M * K * sizeof(float),
                        cudaMemcpyHostToDevice));
  CUDA_CHECK(cudaMemcpy(d_B, h_B.data(), K * N * sizeof(float),
                        cudaMemcpyHostToDevice));

  // 1. Define the Exploration Space (C)
  std::vector<KernelConfig> search_space = {
      {16, 16, 16, 16, 16},
      {32, 32, 32, 32, 32},
      {16, 32, 32, 32, 16},
      {32, 16, 32, 16, 32},
      {8, 64, 16, 64, 8} // Valid configurations where TILE_M * TILE_N <= 1024
  };

  // 2. Create the Evaluator
  Evaluator evaluator(d_A, d_B, d_C, M, N, K);

  // 3. Create the Explorer
  BICOExplorer explorer(search_space, evaluator);

  // 4. Define the Budget (Lambda) and run the exploration
  int budget = static_cast<int>(search_space.size()); // Evaluate all
  explorer.explore(budget);

  // Cleanup
  CUDA_CHECK(cudaFree(d_A));
  CUDA_CHECK(cudaFree(d_B));
  CUDA_CHECK(cudaFree(d_C));

  return 0;
}

</content>

<content full_path="Evaluator.cu">
#include "Evaluator.h"
#include "kernel.cuh"
#include <iostream>
#include <random>
#include <vector>

// Helper macro for checking CUDA calls
#define CUDA_CHECK(call)                                                       \
  do {                                                                         \
    cudaError_t err = call;                                                    \
    if (err != cudaSuccess) {                                                  \
      fprintf(stderr, "CUDA Error at %s:%d: %s\n", __FILE__, __LINE__,         \
              cudaGetErrorString(err));                                        \
      exit(EXIT_FAILURE);                                                      \
    }                                                                          \
  } while (0)

Evaluator::Evaluator(float *d_A_, const float *d_B_, float *d_C_, int M_,
                     int N_, int K_)
    : d_A(d_A_), d_B(d_B_), d_C(d_C_), M(M_), N(N_), K(K_) {}

// This dispatcher function is the magic. It converts a runtime config
// into a compile-time template instantiation.
// NOTE: This leads to a large binary, as one kernel version is compiled for
// each case.
void launch_kernel_with_config(const KernelConfig &config, float *d_C,
                               const float *d_A, const float *d_B, int M, int N,
                               int K) {
  dim3 blockDim(config.BLOCK_DIM_X, config.BLOCK_DIM_Y);
  dim3 gridDim((N + config.TILE_N - 1) / config.TILE_N,
               (M + config.TILE_M - 1) / config.TILE_M);
  size_t shared_size = (static_cast<size_t>(config.TILE_M) * config.TILE_K +
                        static_cast<size_t>(config.TILE_K) * config.TILE_N) *
                       sizeof(float);

  if (config.TILE_M == 16 && config.TILE_N == 16 && config.TILE_K == 16 &&
      config.BLOCK_DIM_X == 16 && config.BLOCK_DIM_Y == 16) {
    fused_glu_kernel<16, 16, 16, 16, 16>
        <<<gridDim, blockDim, shared_size>>>(d_C, d_A, d_B, M, N, K);
  } else if (config.TILE_M == 32 && config.TILE_N == 32 &&
             config.TILE_K == 32 && config.BLOCK_DIM_X == 32 &&
             config.BLOCK_DIM_Y == 32) {
    fused_glu_kernel<32, 32, 32, 32, 32>
        <<<gridDim, blockDim, shared_size>>>(d_C, d_A, d_B, M, N, K);
  } else if (config.TILE_M == 16 && config.TILE_N == 32 &&
             config.TILE_K == 32 && config.BLOCK_DIM_X == 32 &&
             config.BLOCK_DIM_Y == 16) {
    fused_glu_kernel<16, 32, 32, 32, 16>
        <<<gridDim, blockDim, shared_size>>>(d_C, d_A, d_B, M, N, K);
  } else if (config.TILE_M == 32 && config.TILE_N == 16 &&
             config.TILE_K == 32 && config.BLOCK_DIM_X == 16 &&
             config.BLOCK_DIM_Y == 32) {
    fused_glu_kernel<32, 16, 32, 16, 32>
        <<<gridDim, blockDim, shared_size>>>(d_C, d_A, d_B, M, N, K);
  } else if (config.TILE_M == 8 && config.TILE_N == 64 && config.TILE_K == 16 &&
             config.BLOCK_DIM_X == 64 && config.BLOCK_DIM_Y == 8) {
    fused_glu_kernel<8, 64, 16, 64, 8>
        <<<gridDim, blockDim, shared_size>>>(d_C, d_A, d_B, M, N, K);
  } else {
    std::cerr << "Unsupported kernel configuration: " << config.toString()
              << std::endl;
    exit(1);
  }
  CUDA_CHECK(cudaGetLastError());
  CUDA_CHECK(cudaDeviceSynchronize());
}

// The public evaluate function which maps c -> L(c)
double Evaluator::evaluate(const KernelConfig &config) {
  cudaEvent_t start, stop;
  CUDA_CHECK(cudaEventCreate(&start));
  CUDA_CHECK(cudaEventCreate(&stop));

  // Warm-up run
  launch_kernel_with_config(config, d_C, d_A, d_B, M, N, K);

  // Timed run
  CUDA_CHECK(cudaEventRecord(start));
  const int num_runs = 100;
  for (int i = 0; i < num_runs; ++i) {
    launch_kernel_with_config(config, d_C, d_A, d_B, M, N, K);
  }
  CUDA_CHECK(cudaEventRecord(stop));
  CUDA_CHECK(cudaEventSynchronize(stop));

  float milliseconds = 0;
  CUDA_CHECK(cudaEventElapsedTime(&milliseconds, start, stop));
  CUDA_CHECK(cudaEventDestroy(start));
  CUDA_CHECK(cudaEventDestroy(stop));

  return static_cast<double>(milliseconds) / num_runs;
}

</content>

<content full_path="BICOExplorer.cu">
#include "BICOExplorer.h"
#include <iostream>
#include <limits>
#include <random>
#include <vector>

BICOExplorer::BICOExplorer(std::vector<KernelConfig> search_space,
                           Evaluator evaluator)
    : exploration_space_(std::move(search_space)),
      best_latency_(std::numeric_limits<double>::max()),
      evaluator_(std::move(evaluator)) {}

void BICOExplorer::explore(int max_budget) {
  std::cout << "Starting BICO exploration with budget: " << max_budget
            << std::endl;
  std::cout << "Exploration space size: " << exploration_space_.size()
            << std::endl;

  // Simple random search for this example
  std::random_device rd;
  std::mt19937 g(rd());
  std::shuffle(exploration_space_.begin(), exploration_space_.end(), g);
  int budget =
      std::min(static_cast<int>(exploration_space_.size()), max_budget);
  for (int n = 1; n <= budget; ++n) {
    KernelConfig current_config = exploration_space_[n - 1];

    // ---- Spend budget to get an outcome ----
    double current_latency = evaluator_.evaluate(current_config);
    // ---- Update the state of the BICO system ----
    bool is_new_best = false;
    if (current_latency < best_latency_) {
      best_latency_ = current_latency;
      best_config_ = current_config;
      is_new_best = true;
    } else {
      // This configuration "dies" and goes into the sink
      information_sink_.push_back(current_config);
    }
    // ---- Report the current state ----
    std::cout << "--- Budget n = " << n << " ---\n"
              << " Tested: " << current_config.toString() << " -> "
              << current_latency << " ms\n"
              << " Best Latency Guaranteed (x_n*): " << best_latency_ << " ms\n"
              << " Sink Size: " << information_sink_.size() << "\n";
    if (is_new_best) {
      std::cout << " ** New best configuration found! **\n";
    }
  }

  std::cout << "\n===== Exploration Finished =====\n"
            << "Optimal configuration found: " << best_config_.toString()
            << "\n"
            << "With guaranteed latency: " << best_latency_ << " ms\n";
}

</content>

</repo-to-text>
