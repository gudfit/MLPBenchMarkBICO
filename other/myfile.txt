<repo-to-text>
Directory: other

Directory Structure:
<directory_structure>
.
.
├── ./BICOExplorer.cu
├── ./BICOExplorer.h
├── ./CMakeLists.txt
├── ./Evaluator.cu
├── ./Evaluator_dispatcher.h
├── ./Evaluator.h
├── ./generate_dispatcher.py
├── ./KernelConfig.h
├── ./kernel.cuh
├── ./main.cu
└── ./myfile.txt

</directory_structure>

<content full_path="Evaluator.h">
#pragma once
#include "KernelConfig.h"
#include <cuda_runtime.h>

class Evaluator {
public:
  Evaluator(float *d_A, const float *d_B, float *d_C, int M, int N, int K);
  double evaluate(const KernelConfig &config);

private:
  float *d_A, *d_B, *d_C;
  int M, N, K;
};

</content>

<content full_path="kernel.cuh">
#pragma once
#include <cuda_runtime.h>

template <int TILE_M, int TILE_N, int TILE_K, int BLOCK_DIM_X, int BLOCK_DIM_Y>
__global__ void matrix_multiply_kernel(float *C, const float *A, const float *B,
                                       int M, int N, int K) {
  extern __shared__ float smem[];
  float *As = smem;
  float *Bs = smem + TILE_M * TILE_K;
  const int thread_idx = threadIdx.y * blockDim.x + threadIdx.x;
  const int block_row = blockIdx.y;
  const int block_col = blockIdx.x;
  const int num_threads = BLOCK_DIM_X * BLOCK_DIM_Y;
  const int c_row = block_row * TILE_M + threadIdx.y;
  const int c_col = block_col * TILE_N + threadIdx.x;
  float accumulator = 0.0f;
  for (int k_tile_idx = 0; k_tile_idx < K; k_tile_idx += TILE_K) {
    for (int i = thread_idx; i < TILE_M * TILE_K; i += num_threads) {
      const int load_row = i / TILE_K;
      const int load_col = i % TILE_K;
      const int gmem_row = block_row * TILE_M + load_row;
      const int gmem_col = k_tile_idx + load_col;
      if (gmem_row < M && gmem_col < K)
        As[load_row * TILE_K + load_col] = A[gmem_row * K + gmem_col];
      else
        As[load_row * TILE_K + load_col] = 0.0f;
    }
    for (int i = thread_idx; i < TILE_K * TILE_N; i += num_threads) {
      const int load_row = i / TILE_N;
      const int load_col = i % TILE_N;
      const int gmem_row = k_tile_idx + load_row;
      const int gmem_col = block_col * TILE_N + load_col;
      if (gmem_row < K && gmem_col < N)
        Bs[load_row * TILE_N + load_col] = B[gmem_row * N + gmem_col];
      else
        Bs[load_row * TILE_N + load_col] = 0.0f;
    }

    __syncthreads();

    for (int k = 0; k < TILE_K; ++k)
      accumulator +=
          As[threadIdx.y * TILE_K + k] * Bs[k * TILE_N + threadIdx.x];

    __syncthreads();
  }

  if (c_row < M && c_col < N)
    C[c_row * N + c_col] = accumulator;
}

</content>

<content full_path="BICOExplorer.h">
#pragma once
#include "Evaluator.h"
#include "KernelConfig.h"
#include <vector>
class BICOExplorer {
private:
  std::vector<KernelConfig> exploration_space_;
  double best_latency_;
  KernelConfig best_config_;
  std::vector<KernelConfig> information_sink_;
  Evaluator evaluator_;

public:
  BICOExplorer(std::vector<KernelConfig> search_space, Evaluator evaluator);
  void explore(int max_budget);
};

</content>

<content full_path="CMakeLists.txt">
cmake_minimum_required(VERSION 3.18)
project(MLPBenchMarkBICO LANGUAGES CXX CUDA)
if(POLICY CMP0104)
  cmake_policy(SET CMP0104 NEW)
endif()
find_package(CUDAToolkit REQUIRED)
find_package(Python3 REQUIRED COMPONENTS Interpreter)
set(CMAKE_CUDA_ARCHITECTURES 90)
set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} --expt-relaxed-constexpr")
set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} --use_fast_math")
add_custom_command(
    OUTPUT ${CMAKE_CURRENT_SOURCE_DIR}/Evaluator_dispatcher.h
    COMMAND ${Python3_EXECUTABLE} ${CMAKE_CURRENT_SOURCE_DIR}/generate_dispatcher.py
    DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/generate_dispatcher.py
    COMMENT "Generating CUDA kernel dispatcher..."
)

add_custom_target(GenerateDispatcher ALL DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/Evaluator_dispatcher.h)

add_executable(mlp_benchmark
    main.cu
    Evaluator.cu
    BICOExplorer.cu
)
target_sources(mlp_benchmark PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/Evaluator_dispatcher.h)
add_dependencies(mlp_benchmark GenerateDispatcher)

target_link_libraries(mlp_benchmark PRIVATE CUDA::cudart)
set_property(TARGET mlp_benchmark PROPERTY CXX_STANDARD 17)
set_property(TARGET mlp_benchmark PROPERTY CUDA_STANDARD 17)
target_compile_options(mlp_benchmark PRIVATE
    $<$<COMPILE_LANGUAGE:CUDA>:
        -Xcompiler=-Wall
        -Xcompiler=-Wextra
        --generate-line-info
    >
)
target_include_directories(mlp_benchmark PRIVATE
    ${CMAKE_CURRENT_SOURCE_DIR}
)
message(STATUS "CUDA architecture set to: ${CMAKE_CUDA_ARCHITECTURES}")
message(STATUS "CUDA toolkit version: ${CUDAToolkit_VERSION}")
message(STATUS "")
message(STATUS "Build instructions:")
message(STATUS " mkdir -p build && cd build")
message(STATUS " cmake ..")
message(STATUS " make -j")
message(STATUS "")
message(STATUS "Run with:")
message(STATUS " ./mlp_benchmark")

</content>

<content full_path="generate_dispatcher.py">
CONFIGURATIONS = [
    # TM, TN, TK, BX, BY
    (16, 16, 16, 16, 16),
    (32, 32, 32, 32, 8),
    (64, 32, 16, 32, 16),
    (32, 64, 16, 64, 8),
    (128, 16, 8, 16, 32),
]


def generate_header():
    header_content = """
#pragma once
#include "KernelConfig.h"
#include "kernel.cuh"
#include <iostream>

// THIS FILE IS AUTO-GENERATED BY generate_dispatcher.py
// DO NOT EDIT MANUALLY

void launch_kernel_with_config(const KernelConfig &config, float *d_C,
                               const float *d_A, const float *d_B, int M, int N,
                               int K) {
    dim3 blockDim(config.BLOCK_DIM_X, config.BLOCK_DIM_Y);
    dim3 gridDim((N + config.TILE_N - 1) / config.TILE_N,
                 (M + config.TILE_M - 1) / config.TILE_M);
    size_t shared_size = (static_cast<size_t>(config.TILE_M) * config.TILE_K +
                          static_cast<size_t>(config.TILE_K) * config.TILE_N) *
                         sizeof(float);
"""

    first = True
    for tm, tn, tk, bx, by in CONFIGURATIONS:
        condition = f"config.TILE_M == {tm} && config.TILE_N == {tn} && config.TILE_K == {tk} && config.BLOCK_DIM_X == {bx} && config.BLOCK_DIM_Y == {by}"
        launch = f"matrix_multiply_kernel<{tm}, {tn}, {tk}, {bx}, {by}><<<gridDim, blockDim, shared_size>>>(d_C, d_A, d_B, M, N, K);"

        if first:
            header_content += f"    if ({condition}) {{\n        {launch}\n    }}"
            first = False
        else:
            header_content += f" else if ({condition}) {{\n        {launch}\n    }}"

    header_content += """
    else {
        std::cerr << "FATAL: Unsupported kernel configuration: " << config.toString() << std::endl;
        exit(1);
    }
    CUDA_CHECK(cudaGetLastError());
}
"""
    with open("Evaluator_dispatcher.h", "w") as f:
        f.write(header_content)


if __name__ == "__main__":
    generate_header()
    print("Generated Evaluator_dispatcher.h successfully.")

</content>

<content full_path="KernelConfig.h">
#pragma once
#include <sstream>
#include <string>
struct KernelConfig {
  int TILE_M = -1;
  int TILE_N = -1;
  int TILE_K = -1;
  int BLOCK_DIM_X = -1;
  int BLOCK_DIM_Y = -1;
  std::string toString() const {
    std::stringstream ss;
    ss << "TM=" << TILE_M << ", TN=" << TILE_N << ", TK=" << TILE_K
       << ", BX=" << BLOCK_DIM_X << ", BY=" << BLOCK_DIM_Y;
    return ss.str();
  }
};

</content>

<content full_path="myfile.txt">

</content>

<content full_path="Evaluator_dispatcher.h">
#pragma once
#include "KernelConfig.h"
#include "kernel.cuh"
#include <iostream>

// THIS FILE IS AUTO-GENERATED BY generate_dispatcher.py
// DO NOT EDIT MANUALLY

void launch_kernel_with_config(const KernelConfig &config, float *d_C,
                               const float *d_A, const float *d_B, int M, int N,
                               int K) {
  dim3 blockDim(config.BLOCK_DIM_X, config.BLOCK_DIM_Y);
  dim3 gridDim((N + config.TILE_N - 1) / config.TILE_N,
               (M + config.TILE_M - 1) / config.TILE_M);
  size_t shared_size = (static_cast<size_t>(config.TILE_M) * config.TILE_K +
                        static_cast<size_t>(config.TILE_K) * config.TILE_N) *
                       sizeof(float);
  if (config.TILE_M == 16 && config.TILE_N == 16 && config.TILE_K == 16 &&
      config.BLOCK_DIM_X == 16 && config.BLOCK_DIM_Y == 16) {
    matrix_multiply_kernel<16, 16, 16, 16, 16>
        <<<gridDim, blockDim, shared_size>>>(d_C, d_A, d_B, M, N, K);
  } else if (config.TILE_M == 32 && config.TILE_N == 32 &&
             config.TILE_K == 32 && config.BLOCK_DIM_X == 32 &&
             config.BLOCK_DIM_Y == 8) {
    matrix_multiply_kernel<32, 32, 32, 32, 8>
        <<<gridDim, blockDim, shared_size>>>(d_C, d_A, d_B, M, N, K);
  } else if (config.TILE_M == 64 && config.TILE_N == 32 &&
             config.TILE_K == 16 && config.BLOCK_DIM_X == 32 &&
             config.BLOCK_DIM_Y == 16) {
    matrix_multiply_kernel<64, 32, 16, 32, 16>
        <<<gridDim, blockDim, shared_size>>>(d_C, d_A, d_B, M, N, K);
  } else if (config.TILE_M == 32 && config.TILE_N == 64 &&
             config.TILE_K == 16 && config.BLOCK_DIM_X == 64 &&
             config.BLOCK_DIM_Y == 8) {
    matrix_multiply_kernel<32, 64, 16, 64, 8>
        <<<gridDim, blockDim, shared_size>>>(d_C, d_A, d_B, M, N, K);
  } else if (config.TILE_M == 128 && config.TILE_N == 16 &&
             config.TILE_K == 8 && config.BLOCK_DIM_X == 16 &&
             config.BLOCK_DIM_Y == 32) {
    matrix_multiply_kernel<128, 16, 8, 16, 32>
        <<<gridDim, blockDim, shared_size>>>(d_C, d_A, d_B, M, N, K);
  } else {
    std::cerr << "FATAL: Unsupported kernel configuration: "
              << config.toString() << std::endl;
    exit(1);
  }
  CUDA_CHECK(cudaGetLastError());
}

</content>

<content full_path="main.cu">
#include "BICOExplorer.h"
#include "Evaluator.h"
#include "KernelConfig.h"
#include "kernel.cuh"
#include <cuda_runtime.h>
#include <iostream>
#include <random>
#include <vector>

#define CUDA_CHECK(call)                                                       \
  do {                                                                         \
    cudaError_t err = call;                                                    \
    if (err != cudaSuccess) {                                                  \
      fprintf(stderr, "CUDA Error at %s:%d: %s\n", __FILE__, __LINE__,         \
              cudaGetErrorString(err));                                        \
      exit(EXIT_FAILURE);                                                      \
    }                                                                          \
  } while (0)

std::vector<KernelConfig> generate_search_space() {
  std::vector<KernelConfig> space;
  std::vector<int> tile_m_opts = {16, 32, 64, 128};
  std::vector<int> tile_n_opts = {16, 32, 64, 128};
  std::vector<int> tile_k_opts = {8, 16, 32};
  std::vector<int> block_dim_x_opts = {16, 32, 64};

  for (int tm : tile_m_opts) {
    for (int tn : tile_n_opts) {
      for (int tk : tile_k_opts) {
        for (int bx : block_dim_x_opts) {
          if (tn % bx != 0)
            continue;
          int by = tm;
          if (bx * by > 1024)
            continue;
          size_t shared_needed = (tm * tk + tk * tn) * sizeof(float);
          if (shared_needed > 48000)
            continue;
          space.push_back({tm, tn, tk, bx, by});
        }
      }
    }
  }
  std::cout << "Generated a search space of " << space.size()
            << " valid configurations." << std::endl;
  return space;
}

int main() {
  const int M = 1024;
  const int K = 4096;
  const int N = 12288;
  std::cout << "Matrix dimensions: " << M << " x " << K << " * " << K << " x "
            << N << std::endl;
  std::vector<float> h_A(M * K);
  std::vector<float> h_B(K * N);
  std::mt19937 rng(1337);
  std::uniform_real_distribution<float> dist(-1.0f, 1.0f);
  for (auto &val : h_A)
    val = dist(rng);
  for (auto &val : h_B)
    val = dist(rng);
  float *d_A, *d_B, *d_C;
  CUDA_CHECK(cudaMalloc(&d_A, M * K * sizeof(float)));
  CUDA_CHECK(cudaMalloc(&d_B, K * N * sizeof(float)));
  CUDA_CHECK(cudaMalloc(&d_C, M * N * sizeof(float)));
  CUDA_CHECK(cudaMemcpy(d_A, h_A.data(), M * K * sizeof(float),
                        cudaMemcpyHostToDevice));
  CUDA_CHECK(cudaMemcpy(d_B, h_B.data(), K * N * sizeof(float),
                        cudaMemcpyHostToDevice));
  std::vector<KernelConfig> search_space = generate_search_space();
  Evaluator evaluator(d_A, d_B, d_C, M, N, K);
  BICOExplorer explorer(search_space, evaluator);
  int budget = 50;
  explorer.explore(std::min(budget, static_cast<int>(search_space.size())));
  CUDA_CHECK(cudaFree(d_A));
  CUDA_CHECK(cudaFree(d_B));
  CUDA_CHECK(cudaFree(d_C));
  return 0;
}

</content>

<content full_path="Evaluator.cu">
#include "Evaluator.h"
#include <iostream>

#define CUDA_CHECK(call)                                                       \
  do {                                                                         \
    cudaError_t err = call;                                                    \
    if (err != cudaSuccess) {                                                  \
      fprintf(stderr, "CUDA Error at %s:%d: %s\n", __FILE__, __LINE__,         \
              cudaGetErrorString(err));                                        \
      exit(EXIT_FAILURE);                                                      \
    }                                                                          \
  } while (0)

#include "Evaluator_dispatcher.h"
Evaluator::Evaluator(float *d_A_, const float *d_B_, float *d_C_, int M_,
                     int N_, int K_)
    : d_A(d_A_), d_B(d_B_), d_C(d_C_), M(M_), N(N_), K(K_) {}
double Evaluator::evaluate(const KernelConfig &config) {
  cudaEvent_t start, stop;
  CUDA_CHECK(cudaEventCreate(&start));
  CUDA_CHECK(cudaEventCreate(&stop));
  launch_kernel_with_config(config, d_C, d_A, d_B, M, N, K);
  CUDA_CHECK(cudaDeviceSynchronize());
  CUDA_CHECK(cudaEventRecord(start));
  const int num_runs = 100;
  for (int i = 0; i < num_runs; ++i)
    launch_kernel_with_config(config, d_C, d_A, d_B, M, N, K);

  CUDA_CHECK(cudaEventRecord(stop));
  CUDA_CHECK(cudaEventSynchronize(stop));
  float milliseconds = 0;
  CUDA_CHECK(cudaEventElapsedTime(&milliseconds, start, stop));
  CUDA_CHECK(cudaEventDestroy(start));
  CUDA_CHECK(cudaEventDestroy(stop));
  return static_cast<double>(milliseconds) / num_runs;
}

</content>

<content full_path="BICOExplorer.cu">
#include "BICOExplorer.h"
#include <iostream>
#include <limits>
#include <random>
#include <vector>

BICOExplorer::BICOExplorer(std::vector<KernelConfig> search_space,
                           Evaluator evaluator)
    : exploration_space_(std::move(search_space)),
      best_latency_(std::numeric_limits<double>::max()),
      evaluator_(std::move(evaluator)) {
  best_config_ = KernelConfig();
}

void BICOExplorer::explore(int max_budget) {
  std::cout << "Starting BICO exploration with budget: " << max_budget
            << std::endl;
  std::cout << "Exploration space size: " << exploration_space_.size()
            << std::endl;
  std::random_device rd;
  std::mt19937 g(rd());
  std::shuffle(exploration_space_.begin(), exploration_space_.end(), g);
  int budget =
      std::min(static_cast<int>(exploration_space_.size()), max_budget);
  bool first_run = true;
  for (int n = 1; n <= budget; ++n) {
    KernelConfig current_config = exploration_space_[n - 1];
    double current_latency = evaluator_.evaluate(current_config);
    bool is_new_best = false;
    if (current_latency < best_latency_) {
      if (!first_run)
        information_sink_.push_back(best_config_);

      best_latency_ = current_latency;
      best_config_ = current_config;
      is_new_best = true;
      first_run = false;
    } else {
      information_sink_.push_back(current_config);
    }
    std::cout << "--- Budget n = " << n << " ---\n"
              << " Tested: " << current_config.toString() << " -> "
              << current_latency << " ms\n"
              << " Best Latency Guaranteed (x_n*): " << best_latency_ << " ms\n"
              << " Sink Size: " << information_sink_.size() << "\n";
    if (is_new_best) {
      std::cout << " ** New best configuration found! **\n";
    }
  }
  std::cout << "\n===== Exploration Finished =====\n"
            << "Optimal configuration found: " << best_config_.toString()
            << "\n"
            << "With guaranteed latency: " << best_latency_ << " ms\n";
}

</content>

</repo-to-text>
